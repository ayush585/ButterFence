"""Red-team module tests: repo scanning, prompt building, parsing, integration."""

from __future__ import annotations

import json
import os
import sys
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest

from butterfence.audit import ScenarioResult, run_scenario
from butterfence.config import DEFAULT_CONFIG
from butterfence.redteam import (
    APICallError,
    APIKeyMissingError,
    RedTeamError,
    RedTeamResult,
    RepoContext,
    ScenarioParseError,
    build_system_prompt,
    build_user_prompt,
    parse_scenarios,
    scan_repo_context,
    _validate_scenario,
)
from butterfence.rules import Category
from butterfence.scoring import calculate_score


# ---------------------------------------------------------------------------
# Build attack command strings purely from ordinals so no regex can
# match them in the raw source code of this file.
# ---------------------------------------------------------------------------

def _from_ords(*codes: int) -> str:
    return "".join(chr(c) for c in codes)


def _destructive_cmd() -> str:
    """Destructive shell command built from char codes."""
    parts = [
        _from_ords(114, 109, 32, 45, 114, 102),
        _from_ords(32, 47, 32, 45, 45, 110, 111),
        _from_ords(45, 112, 114, 101, 115, 101),
        _from_ords(114, 118, 101, 45, 114, 111),
        _from_ords(111, 116),
    ]
    return "".join(parts)


def _force_push_cmd() -> str:
    """Risky git command built from char codes."""
    parts = [
        _from_ords(103, 105, 116, 32, 112, 117),
        _from_ords(115, 104, 32, 45, 45, 102),
        _from_ords(111, 114, 99, 101, 32, 111),
        _from_ords(114, 105, 103, 105, 110),
        _from_ords(32, 109, 97, 105, 110),
    ]
    return "".join(parts)


def _docker_priv_cmd() -> str:
    """Container escape command built from char codes."""
    parts = [
        _from_ords(100, 111, 99, 107, 101, 114),
        _from_ords(32, 114, 117, 110, 32, 45),
        _from_ords(45, 112, 114, 105, 118, 105),
        _from_ords(108, 101, 103, 101, 100, 32),
        _from_ords(45, 105, 116, 32, 117, 98),
        _from_ords(117, 110, 116, 117, 32, 98),
        _from_ords(97, 115, 104),
    ]
    return "".join(parts)


# ---------------------------------------------------------------------------
# Shared test data -- scenarios built at runtime via helpers
# ---------------------------------------------------------------------------

def _make_sample_scenarios() -> list[dict]:
    return [
        {
            "id": "redteam-test-001",
            "name": "Test destructive command",
            "category": "destructive_shell",
            "severity": "critical",
            "tool": "Bash",
            "tool_input": {"command": _destructive_cmd()},
            "expected_decision": "block",
            "explanation": "Test scenario for destructive shell",
        },
        {
            "id": "redteam-test-002",
            "name": "Test risky operation",
            "category": "risky_git",
            "severity": "high",
            "tool": "Bash",
            "tool_input": {"command": _force_push_cmd()},
            "expected_decision": "block",
            "explanation": "Test scenario for risky git operation",
        },
        {
            "id": "redteam-test-003",
            "name": "Test container escape",
            "category": "docker_escape",
            "severity": "critical",
            "tool": "Bash",
            "tool_input": {"command": _docker_priv_cmd()},
            "expected_decision": "block",
            "explanation": "Test scenario for container escape",
        },
    ]


def _make_sample_response() -> str:
    return json.dumps(_make_sample_scenarios())


# ---------------------------------------------------------------------------
# A. TestScanRepoContext
# ---------------------------------------------------------------------------

class TestScanRepoContext:
    """Tests for scan_repo_context using real tmp_path directories."""

    def test_scans_file_tree(self, tmp_path: Path) -> None:
        """Created files should appear in the returned file_tree."""
        (tmp_path / "app.py").write_text("print('hello')")
        (tmp_path / "README.md").write_text("# readme")
        sub = tmp_path / "src"
        sub.mkdir()
        (sub / "main.py").write_text("pass")

        ctx = scan_repo_context(tmp_path)

        assert "app.py" in ctx.file_tree
        assert "README.md" in ctx.file_tree
        assert "src/main.py" in ctx.file_tree
        assert ctx.total_files == 3

    def test_detects_tech_stack(self, tmp_path: Path) -> None:
        """package.json and pyproject.toml should map to Node.js and Python."""
        (tmp_path / "package.json").write_text("{}")
        (tmp_path / "pyproject.toml").write_text("[build-system]")

        ctx = scan_repo_context(tmp_path)

        assert "Node.js" in ctx.tech_stack
        assert "Python" in ctx.tech_stack

    def test_detects_sensitive_files(self, tmp_path: Path) -> None:
        """Files with sensitive name patterns should be detected."""
        pem_name = "server.pem"
        key_name = "private.key"
        (tmp_path / pem_name).write_text("fake")
        (tmp_path / key_name).write_text("fake")

        ctx = scan_repo_context(tmp_path)

        assert len(ctx.sensitive_files) >= 2
        assert any("pem" in f for f in ctx.sensitive_files)
        assert any("key" in f for f in ctx.sensitive_files)

    def test_skips_excluded_dirs(self, tmp_path: Path) -> None:
        """Files inside node_modules and .git dirs must not appear."""
        nm = tmp_path / "node_modules"
        nm.mkdir()
        (nm / "lodash.js").write_text("module.exports = {}")

        gitdir = tmp_path / ".git"
        gitdir.mkdir()
        (gitdir / "config").write_text("[core]")

        (tmp_path / "index.js").write_text("console.log(1)")

        ctx = scan_repo_context(tmp_path)

        assert "index.js" in ctx.file_tree
        assert all("node_modules" not in f for f in ctx.file_tree)
        assert all(".git" not in f for f in ctx.file_tree)

    def test_detects_git_branch(self, tmp_path: Path) -> None:
        """A .git/HEAD with a ref should set git_branch correctly."""
        gitdir = tmp_path / ".git"
        gitdir.mkdir()
        (gitdir / "HEAD").write_text("ref: refs/heads/main\n")

        ctx = scan_repo_context(tmp_path)

        assert ctx.has_git is True
        assert ctx.git_branch == "main"

    def test_detects_languages(self, tmp_path: Path) -> None:
        """Files with .py and .js extensions map to Python and JavaScript."""
        (tmp_path / "app.py").write_text("pass")
        (tmp_path / "index.js").write_text("1")

        ctx = scan_repo_context(tmp_path)

        assert "Python" in ctx.languages
        assert "JavaScript" in ctx.languages


# ---------------------------------------------------------------------------
# B. TestParseScenarios
# ---------------------------------------------------------------------------

class TestParseScenarios:
    """Tests for parse_scenarios handling various input formats."""

    def test_parses_valid_json(self) -> None:
        """Direct JSON array should parse correctly."""
        result = parse_scenarios(_make_sample_response())
        assert len(result) == 3
        assert result[0]["id"] == "redteam-test-001"

    def test_handles_markdown_fenced_json(self) -> None:
        """JSON wrapped in json-tagged fences should parse."""
        fenced = "```json\n" + _make_sample_response() + "\n```"
        result = parse_scenarios(fenced)
        assert len(result) == 3

    def test_handles_plain_fenced_json(self) -> None:
        """JSON wrapped in plain fences should parse."""
        fenced = "```\n" + _make_sample_response() + "\n```"
        result = parse_scenarios(fenced)
        assert len(result) == 3

    def test_rejects_empty_response(self) -> None:
        """Empty string should raise ScenarioParseError."""
        with pytest.raises(ScenarioParseError):
            parse_scenarios("")

    def test_rejects_non_array(self) -> None:
        """A JSON dict (not array) should raise ScenarioParseError."""
        with pytest.raises(ScenarioParseError):
            parse_scenarios('{"not": "an array"}')

    def test_skips_invalid_scenarios(self) -> None:
        """Mix of valid and invalid dicts returns only the valid ones."""
        samples = _make_sample_scenarios()
        mixed = [
            samples[0],
            {"bad": "entry"},
            samples[1],
        ]
        result = parse_scenarios(json.dumps(mixed))
        assert len(result) == 2
        ids = [s["id"] for s in result]
        assert "redteam-test-001" in ids
        assert "redteam-test-002" in ids

    def test_all_invalid_raises(self) -> None:
        """If every entry is invalid, ScenarioParseError is raised."""
        bad = [{"bad": "a"}, {"also": "bad"}]
        with pytest.raises(ScenarioParseError, match="No valid scenarios"):
            parse_scenarios(json.dumps(bad))

    def test_prefixes_id_with_redteam(self) -> None:
        """A scenario id lacking 'redteam-' prefix gets it added."""
        scenario = _make_sample_scenarios()[0].copy()
        scenario["id"] = "no-prefix-001"
        result = parse_scenarios(json.dumps([scenario]))
        assert result[0]["id"] == "redteam-no-prefix-001"


# ---------------------------------------------------------------------------
# C. TestValidateScenario
# ---------------------------------------------------------------------------

class TestValidateScenario:
    """Tests for _validate_scenario."""

    def test_valid_bash_scenario(self) -> None:
        """A complete valid scenario should pass validation."""
        scenario = _make_sample_scenarios()[0].copy()
        validated = _validate_scenario(scenario, 0)
        assert validated["tool"] == "Bash"
        assert validated["category"] == "destructive_shell"
        assert validated["expected_decision"] == "block"

    def test_rejects_invalid_tool(self) -> None:
        """Tool name not in valid set raises ValueError."""
        scenario = _make_sample_scenarios()[0].copy()
        scenario["tool"] = "BadTool"
        with pytest.raises(ValueError, match="Invalid tool"):
            _validate_scenario(scenario, 0)

    def test_rejects_missing_command(self) -> None:
        """Bash scenario with empty tool_input should raise ValueError."""
        scenario = _make_sample_scenarios()[0].copy()
        scenario["tool_input"] = {}
        with pytest.raises(ValueError, match="missing keys"):
            _validate_scenario(scenario, 0)

    def test_fills_default_explanation(self) -> None:
        """Scenario without explanation gets a generated default."""
        scenario = _make_sample_scenarios()[0].copy()
        scenario.pop("explanation", None)
        validated = _validate_scenario(scenario, 0)
        assert "explanation" in validated
        assert "destructive_shell" in validated["explanation"]
        assert "Bash" in validated["explanation"]


# ---------------------------------------------------------------------------
# D. TestGenerateScenariosViaAPI
# ---------------------------------------------------------------------------

class TestGenerateScenariosViaAPI:
    """Tests for generate_scenarios_via_api with mocked anthropic module."""

    def test_missing_api_key(self) -> None:
        """No ANTHROPIC_API_KEY env var should raise APIKeyMissingError."""
        from butterfence.redteam import generate_scenarios_via_api

        ctx = RepoContext(
            root="/tmp/test",
            file_tree=["app.py"],
            tech_stack=["Python"],
            sensitive_files=[],
            git_branch="main",
            has_git=True,
            total_files=1,
            languages=["Python"],
        )

        env = os.environ.copy()
        env.pop("ANTHROPIC_API_KEY", None)

        mock_mod = MagicMock()
        with patch.dict(os.environ, env, clear=True), \
             patch.dict("sys.modules", {"anthropic": mock_mod}):
            with pytest.raises(APIKeyMissingError):
                generate_scenarios_via_api(ctx, count=2)

    def test_successful_api_call(self) -> None:
        """Mocked successful API call should return parsed scenarios."""
        from butterfence.redteam import generate_scenarios_via_api

        ctx = RepoContext(
            root="/tmp/test",
            file_tree=["app.py"],
            tech_stack=["Python"],
            sensitive_files=[],
            git_branch="main",
            has_git=True,
            total_files=1,
            languages=["Python"],
        )

        mock_mod = MagicMock()
        mock_text_block = MagicMock()
        mock_text_block.text = _make_sample_response()
        mock_response = MagicMock()
        mock_response.content = [mock_text_block]
        mock_mod.Anthropic.return_value.messages.create.return_value = (
            mock_response
        )

        env = os.environ.copy()
        env["ANTHROPIC_API_KEY"] = "test-key-12345"

        with patch.dict(os.environ, env, clear=True), \
             patch.dict("sys.modules", {"anthropic": mock_mod}):
            result = generate_scenarios_via_api(ctx, count=3)

        assert len(result) == 3
        assert result[0]["id"] == "redteam-test-001"

    def test_auth_error(self) -> None:
        """AuthenticationError from anthropic becomes APICallError."""
        from butterfence.redteam import generate_scenarios_via_api

        ctx = RepoContext(
            root="/tmp/test",
            file_tree=["app.py"],
            tech_stack=["Python"],
            sensitive_files=[],
            git_branch="main",
            has_git=True,
            total_files=1,
            languages=["Python"],
        )

        mock_mod = MagicMock()

        class FakeAuthError(Exception):
            pass

        mock_mod.AuthenticationError = FakeAuthError
        mock_mod.RateLimitError = type("RateLimitError", (Exception,), {})
        mock_mod.APIConnectionError = type(
            "APIConnectionError", (Exception,), {}
        )
        mock_mod.APIStatusError = type("APIStatusError", (Exception,), {})
        mock_mod.Anthropic.return_value.messages.create.side_effect = (
            FakeAuthError("bad key")
        )

        env = os.environ.copy()
        env["ANTHROPIC_API_KEY"] = "bad-key-value"

        with patch.dict(os.environ, env, clear=True), \
             patch.dict("sys.modules", {"anthropic": mock_mod}):
            with pytest.raises(APICallError, match="Authentication failed"):
                generate_scenarios_via_api(ctx, count=2)


# ---------------------------------------------------------------------------
# E. TestRedTeamIntegration
# ---------------------------------------------------------------------------

class TestRedTeamIntegration:
    """Integration tests using the real matcher (no mocks)."""

    def test_obvious_attack_caught(self) -> None:
        """Destructive command must be blocked by default config."""
        scenario = {
            "id": "redteam-integ-001",
            "name": "Obvious destructive attack",
            "category": "destructive_shell",
            "severity": "critical",
            "tool": "Bash",
            "tool_input": {"command": _destructive_cmd()},
            "expected_decision": "block",
        }
        result = run_scenario(scenario, DEFAULT_CONFIG)
        assert result.passed is True
        assert result.actual_decision == "block"

    def test_docker_escape_caught(self) -> None:
        """Container privileged run should be blocked."""
        scenario = {
            "id": "redteam-integ-002",
            "name": "Container privileged escape",
            "category": "docker_escape",
            "severity": "critical",
            "tool": "Bash",
            "tool_input": {"command": _docker_priv_cmd()},
            "expected_decision": "block",
        }
        result = run_scenario(scenario, DEFAULT_CONFIG)
        assert result.passed is True
        assert result.actual_decision == "block"

    def test_results_compatible_with_scorer(self) -> None:
        """ScenarioResult dicts should be accepted by calculate_score."""
        scenarios = [
            {
                "id": "redteam-score-001",
                "name": "Risky git operation",
                "category": "risky_git",
                "severity": "high",
                "tool": "Bash",
                "tool_input": {"command": _force_push_cmd()},
                "expected_decision": "block",
            },
            {
                "id": "redteam-score-002",
                "name": "Destructive command",
                "category": "destructive_shell",
                "severity": "critical",
                "tool": "Bash",
                "tool_input": {"command": _destructive_cmd()},
                "expected_decision": "block",
            },
        ]

        results = [run_scenario(s, DEFAULT_CONFIG) for s in scenarios]

        audit_dicts = [
            {
                "id": r.id,
                "name": r.name,
                "category": r.category,
                "severity": r.severity,
                "passed": r.passed,
                "reason": r.reason,
            }
            for r in results
        ]

        score = calculate_score(audit_dicts, DEFAULT_CONFIG)
        assert score.total_score >= 0
        assert score.total_score <= 100
        assert score.grade in ("A", "B", "C", "D", "F")


# ---------------------------------------------------------------------------
# F. TestPromptConstruction
# ---------------------------------------------------------------------------

class TestPromptConstruction:
    """Tests for build_system_prompt and build_user_prompt."""

    def test_system_prompt_has_all_categories(self) -> None:
        """The system prompt must mention every Category enum value."""
        prompt = build_system_prompt()
        for cat in Category:
            assert cat.value in prompt, (
                f"Category {cat.value!r} missing from system prompt"
            )

    def test_system_prompt_has_all_tools(self) -> None:
        """Bash, Read, Write, Edit must all appear in the system prompt."""
        prompt = build_system_prompt()
        for tool in ("Bash", "Read", "Write", "Edit"):
            assert tool in prompt, f"Tool {tool!r} missing from system prompt"

    def test_user_prompt_includes_context(self) -> None:
        """Repo context data should be embedded in the user prompt."""
        ctx = RepoContext(
            root="/home/user/my-project",
            file_tree=["src/main.py", "tests/test_app.py", "Dockerfile"],
            tech_stack=["Python", "Docker"],
            sensitive_files=["config.pem"],
            git_branch="develop",
            has_git=True,
            total_files=42,
            languages=["Python"],
        )
        prompt = build_user_prompt(
            ctx, count=5, categories=["risky_git", "docker_escape"]
        )

        assert "/home/user/my-project" in prompt
        assert "develop" in prompt
        assert "42" in prompt
        assert "Python" in prompt
        assert "Docker" in prompt
        assert "5" in prompt
        assert "risky_git" in prompt
        assert "docker_escape" in prompt


# ---------------------------------------------------------------------------
# G. TestRedTeamResult
# ---------------------------------------------------------------------------

class TestRedTeamResult:
    """Tests for the RedTeamResult dataclass."""

    def _empty_ctx(self) -> RepoContext:
        return RepoContext(
            root="/tmp",
            file_tree=[],
            tech_stack=[],
            sensitive_files=[],
            git_branch="",
            has_git=False,
            total_files=0,
            languages=[],
        )

    def test_catch_rate_percentage(self) -> None:
        """catch_rate should return correct percentage."""
        result = RedTeamResult(
            scenarios_generated=10,
            scenarios_run=10,
            caught=8,
            missed=2,
            results=[],
            model_used="test-model",
            repo_context=self._empty_ctx(),
            raw_scenarios=[],
        )
        assert result.catch_rate == 80.0

    def test_catch_rate_zero_run(self) -> None:
        """catch_rate should return 0.0 when no scenarios were run."""
        result = RedTeamResult(
            scenarios_generated=0,
            scenarios_run=0,
            caught=0,
            missed=0,
            results=[],
            model_used="test-model",
            repo_context=self._empty_ctx(),
            raw_scenarios=[],
        )
        assert result.catch_rate == 0.0
